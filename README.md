# ğŸ¤ Jarvis Audio Pipeline

> âš ï¸ **LOCKED SERVICE** - This service is stable and production-ready. DO NOT modify without explicit user approval.

> **Audio ingestion only.** Monitors Google Drive for voice memos, transcribes using Modal (GPU), saves the transcript, then hands off to the Intelligence Service for analysis.

## ğŸ¯ Role in the Ecosystem

This service does **ONE thing well**: audio â†’ text. It does NOT contain any AI/LLM logic.

```
[Voice Memo] â†’ Download â†’ Transcribe (Modal GPU) â†’ Save Transcript â†’ Call Intelligence Service â†’ Move to Processed
```

**Why no AI here?** All intelligence lives in `jarvis-intelligence-service`. This keeps the pipeline simple, focused, and maintainable.

---

## ğŸ—ï¸ Architecture

### Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Google Drive       â”‚  User drops voice memo
â”‚  (Watched Folder)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ Webhook or Poll
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cloud Run Server   â”‚  cloud_run_server.py (FastAPI)
â”‚  /process/upload    â”‚  Also handles direct uploads from Telegram
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AudioPipeline      â”‚  run_pipeline.py
â”‚  (Orchestrator)     â”‚  Coordinates all steps
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”œâ”€â”€ 1. download_task.py     â†’ Download from Drive
          â”‚
          â”œâ”€â”€ 2. transcribe_task.py   â†’ Send to Modal (GPU)
          â”‚                              â†“
          â”‚                           [WhisperX on A10G GPU]
          â”‚                              â†“
          â”‚                           Return {text, segments, speakers}
          â”‚
          â”œâ”€â”€ 3. analyze_task_multi.py â†’ Save transcript to Supabase
          â”‚                              â†’ Call Intelligence Service
          â”‚
          â”œâ”€â”€ 4. move_task.py          â†’ Move to "Processed" folder
          â”‚
          â””â”€â”€ 5. cleanup_task.py       â†’ Delete temp files
```

### Key Components

| File | Purpose |
|------|---------|
| `cloud_run_server.py` | FastAPI HTTP server (webhooks, direct uploads) |
| `run_pipeline.py` | `AudioPipeline` class - orchestrates all steps |
| `src/tasks/*.py` | Individual pipeline steps (download, transcribe, etc.) |
| `src/core/transcription_backends/` | Multi-backend transcription (Modal, External GPU, Local) |
| `src/supabase/multi_db.py` | Database operations (transcripts, pipeline_logs) |
| `modal_whisperx_v2.py` | Modal function definition (deployed separately) |

---

## ğŸ”Œ API Endpoints

### `GET /health`
Health check for Cloud Run.
```json
{"status": "healthy", "processing": false}
```

### `POST /process`
Process all available audio files in Google Drive.
```bash
# Called by Cloud Scheduler every 5 minutes
curl -X POST https://jarvis-audio-pipeline-xxx.run.app/process
```
**Parameters:**
- `background=true` - Return immediately, process async (for large files)
- `reset=true` - Clear processed files cache to force reprocessing

### `POST /process/upload`
**Primary endpoint for Telegram bot.** Process an uploaded audio file directly.
```bash
curl -X POST -F "file=@voice_memo.ogg" -F "username=bertan" \
  https://jarvis-audio-pipeline-xxx.run.app/process/upload
```
**Response:**
```json
{
  "status": "success",
  "category": "meeting",
  "summary": "ğŸ“… Meeting: Sales call with John\nâœ… 3 task(s) created",
  "details": {
    "transcript_id": "uuid",
    "meeting_ids": ["uuid"],
    "task_ids": ["uuid", "uuid", "uuid"]
  }
}
```

### `POST /webhook/drive`
Google Drive push notification endpoint. Triggered automatically when files are added.

### `POST /renew-webhook`
Renew Google Drive webhook (24h expiry). Called by Cloud Scheduler daily.

### `GET /status`
Current processing status.
```json
{"status": "idle", "processing": false, "pipeline_ready": true}
```

---

## ğŸ“Š Database Schema

### `transcripts` (Supabase)
```sql
id              UUID PRIMARY KEY
source_file     TEXT           -- Original filename
full_text       TEXT           -- Complete transcript
audio_duration  FLOAT          -- Seconds
language        TEXT           -- e.g., "en"
segments        JSONB          -- [{start, end, text, speaker}]
speakers        TEXT[]         -- ["SPEAKER_00", "SPEAKER_01"]
created_at      TIMESTAMPTZ
```

### `pipeline_logs` (Supabase)
```sql
id              UUID PRIMARY KEY
run_id          UUID           -- Groups events in one pipeline run
event_type      TEXT           -- download, transcribe, analyze, complete, error
status          TEXT           -- started, success, error
message         TEXT
source_file     TEXT
duration_ms     INT
details         JSONB
created_at      TIMESTAMPTZ
```

---

## ğŸ™ï¸ Transcription Backends

The pipeline supports **multiple transcription backends** with automatic fallback:

| Backend | When Used | Cost | Speed |
|---------|-----------|------|-------|
| **Modal** | Default (production) | ~$0.10-0.30/hour | Fast (A10G GPU) |
| **External GPU** | If `EXTERNAL_GPU_URL` set | Free | Fastest |
| **Local CPU** | Fallback | Free | Slow |

**Backend Selection Logic** (`src/core/transcription_backends/router.py`):
1. Check if `TRANSCRIPTION_BACKEND` env var forces a specific backend
2. Try External GPU if `EXTERNAL_GPU_URL` is set and reachable
3. Try Modal if authenticated
4. Fall back to local CPU

---

## ğŸš€ Deployment

### Automatic via GitHub (Recommended)
Push to `main` â†’ Cloud Build â†’ Cloud Run (automatic)

```bash
git push origin main
```

Cloud Build triggers:
- **Trigger**: `jarvis-audio-pipeline-deploy`
- **Branch**: `^main$`
- **Config**: `cloudbuild.yaml`

### Environment Variables (Cloud Run)
```
GOOGLE_DRIVE_FOLDER_ID        # Folder to watch
GOOGLE_DRIVE_PROCESSED_FOLDER_ID  # Where to move processed files
INTELLIGENCE_SERVICE_URL      # https://jarvis-intelligence-service-xxx.run.app
SUPABASE_URL                  # Database URL
SUPABASE_KEY                  # Database key
GOOGLE_TOKEN_JSON             # OAuth token (JSON string)
```

### Modal Setup (One-time)
```bash
# Deploy the Modal function
modal deploy modal_whisperx_v2.py
```

---

## ğŸ”§ Local Development

```bash
# Install dependencies
pip install -r requirements.txt

# Copy environment file
cp .env.example .env

# Run locally
python cloud_run_server.py

# Test health
curl http://localhost:8080/health
```

---

## ğŸ” Error Handling

### Retry Logic
- Intelligence Service calls retry **3 times** with exponential backoff
- Modal transcription has built-in retries

### Pipeline Failures
- All events logged to `pipeline_logs` table
- Errors include full stack trace in `message` field
- Temp files cleaned up even on error

### Monitoring
```sql
-- Recent errors
SELECT * FROM pipeline_logs 
WHERE status = 'error' 
ORDER BY created_at DESC 
LIMIT 10;

-- Pipeline success rate
SELECT 
  date_trunc('day', created_at) as day,
  COUNT(*) FILTER (WHERE status = 'success') as success,
  COUNT(*) FILTER (WHERE status = 'error') as errors
FROM pipeline_logs 
WHERE event_type = 'pipeline_complete'
GROUP BY 1 ORDER BY 1 DESC;
```

---

## ğŸ“ Project Structure

```
jarvis-audio-pipeline/
â”œâ”€â”€ cloud_run_server.py    # FastAPI server (entry point)
â”œâ”€â”€ run_pipeline.py        # AudioPipeline orchestrator
â”œâ”€â”€ modal_whisperx_v2.py   # Modal function (deployed separately)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py          # Configuration (env vars)
â”‚   â”œâ”€â”€ tasks/             # Pipeline steps
â”‚   â”‚   â”œâ”€â”€ download_task.py
â”‚   â”‚   â”œâ”€â”€ transcribe_task.py
â”‚   â”‚   â”œâ”€â”€ analyze_task_multi.py
â”‚   â”‚   â”œâ”€â”€ move_task.py
â”‚   â”‚   â””â”€â”€ cleanup_task.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ transcription_backends/  # Multi-backend support
â”‚   â”‚   â””â”€â”€ monitor.py     # Google Drive monitoring
â”‚   â””â”€â”€ supabase/
â”‚       â””â”€â”€ multi_db.py    # Database operations
â”œâ”€â”€ cloudbuild.yaml        # Cloud Build config
â”œâ”€â”€ Dockerfile
â””â”€â”€ requirements.txt
```

---

## âš ï¸ Important Notes

### DO NOT
- âŒ Add AI/LLM logic here (goes in Intelligence Service)
- âŒ Manually deploy (`gcloud builds submit` will fail due to missing secrets)
- âŒ Modify transcription flow without testing Modal integration
- âŒ Store audio files permanently (ephemeral /tmp only)

### DO
- âœ… Push to main for automatic deployment
- âœ… Check `pipeline_logs` table for debugging
- âœ… Use `/process/upload` for Telegram bot integration
- âœ… Renew webhook daily via Cloud Scheduler
